# Pipeline de Datos Escalable y Confiable Pipeline de Datos Escalable y Confiable

## Sobre Este Proyecto

Este proyecto implementa una soluciÃ³n completa de pipeline de datos empresarial que desarrollÃ© como parte de una evaluaciÃ³n tÃ©cnica. El objetivo era crear un sistema escalable y confiable que pudiera manejar grandes volÃºmenes de datos utilizando tecnologÃ­as modernas de la industria.

### Â¿QuÃ© problema resuelve?

En mi experiencia trabajando con datos, he visto cÃ³mo las empresas luchan con:
- **VolÃºmenes crecientes de datos** que saturan sistemas legacy
- **Procesos manuales** propensos a errores humanos
- **Falta de visibilidad** sobre el estado de los pipelines
- **Escalabilidad limitada** cuando el negocio crece

Esta soluciÃ³n aborda estos desafÃ­os implementando un pipeline robusto que puede crecer con las necesidades del negocio.

## Arquitectura del Sistema

He diseÃ±ado una arquitectura de microservicios que separa claramente las responsabilidades:

```
Data Sources â†’ Airflow (Orchestration) â†’ Spark (Processing) â†’ PostgreSQL (Storage)
     â†“              â†“                       â†“                    â†“
  CSV/JSON      Scheduling            Distributed           Data Warehouse
   Files        Monitoring            Computing              Analytics
```

### TecnologÃ­as Principales

- **Apache Airflow 2.7.2**: Para orquestaciÃ³n y monitoreo de workflows
- **Apache Spark 3.4**: Procesamiento distribuido de grandes volÃºmenes
- **PostgreSQL 14**: Base de datos confiable para almacenamiento final
- **Docker Compose**: OrquestaciÃ³n de servicios y deployment
- **Redis**: Message broker para comunicaciÃ³n entre servicios

## Inicio RÃ¡pido

### Requisitos Previos
- Docker Desktop instalado y ejecutÃ¡ndose
- Al menos 8GB de RAM disponible
- Puertos 5432, 6379, 7077, 8080, 8081 libres

### InstalaciÃ³n en 3 Pasos

```bash
# 1. Iniciar todos los servicios
./start.sh

# 2. Verificar que todo funcione
python validate_simple.py

# 3. Acceder a la interfaz web
open http://localhost:8080
```

### Scripts de Utilidad

```bash
# Ver logs en tiempo real
./logs.sh

# Ejecutar validaciÃ³n completa
./run_validation.sh

# Detener servicios
./stop.sh

# Limpieza completa (cuidado: borra datos)
./clean.sh
```

## Interfaces Web

Una vez que el sistema estÃ© ejecutÃ¡ndose:

| Servicio | URL | Credenciales | PropÃ³sito |
|----------|-----|--------------|-----------|
| **Airflow Web UI** | http://localhost:8080 | admin/admin123 | Monitoreo y control del pipeline |
| **Spark Master UI** | http://localhost:8081 | - | Estado del cluster de procesamiento |
| **PostgreSQL** | localhost:5432 | airflow/airflow123 | Base de datos (usar cliente SQL) |

## Pipeline de Datos Implementado

### Flujo de Procesamiento

El DAG `data_pipeline_etl` ejecuta estas etapas secuencialmente:

1. **Data Ingestion**: Lee archivos CSV y JSON desde el directorio `/data/raw/`
2. **Data Cleaning**: Valida formatos, elimina duplicados, maneja valores nulos
3. **Data Transformation**: Calcula mÃ©tricas agregadas y KPIs de negocio
4. **Data Loading**: Inserta datos procesados en PostgreSQL
5. **Quality Checks**: Valida integridad y consistencia de datos
6. **Daily Summary**: Genera reportes con mÃ©tricas del dÃ­a
7. **Notifications**: EnvÃ­a alertas sobre el estado del procesamiento

### CÃ³mo Ejecutar el Pipeline

1. **Accede a Airflow**: http://localhost:8080 (admin/admin123)
2. **Encuentra el DAG**: Busca `data_pipeline_etl` en la lista
3. **ActÃ­valo**: Cambia el toggle de OFF a ON
4. **Ejecuta**: Haz clic en "Trigger DAG" para iniciar manualmente
5. **Monitorea**: Utiliza la vista Graph para ver el progreso en tiempo real

## Estructura del Proyecto

```
â”œâ”€â”€ README.md                      # Este archivo
â”œâ”€â”€ docker-compose.yml            # ConfiguraciÃ³n de servicios
â”œâ”€â”€ Dockerfile.airflow            # Imagen customizada de Airflow
â”œâ”€â”€ requirements.txt              # Dependencias Python
â”œâ”€â”€ start.sh / stop.sh / clean.sh # Scripts de manejo del entorno
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ data_pipeline_dag.py     # DefiniciÃ³n del pipeline principal
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ data_ingestion.py        # MÃ³dulo de lectura de datos
â”‚   â”œâ”€â”€ data_cleaning.py         # MÃ³dulo de limpieza y validaciÃ³n
â”‚   â”œâ”€â”€ data_transformation.py   # MÃ³dulo de transformaciones
â”‚   â””â”€â”€ data_loader.py           # MÃ³dulo de carga a PostgreSQL
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                     # Datos fuente (CSV, JSON)
â”‚   â”œâ”€â”€ processed/               # Resultados del procesamiento
â”‚   â””â”€â”€ logs/                    # Logs del pipeline
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ data_quality_monitor.py # Monitor de calidad de datos
â”‚   â””â”€â”€ alert_system.py         # Sistema de alertas
â””â”€â”€ sql/
    â””â”€â”€ create_tables.sql        # Scripts de base de datos
```

## Datos de Prueba Incluidos

Para facilitar las pruebas, incluÃ­ datasets en tres tamaÃ±os:

### Datasets Small (~20 registros cada uno)
- `sales_data_small.csv`: Datos de ventas bÃ¡sicos
- `customer_data_small.json`: InformaciÃ³n de clientes
- `products_data_small.csv`: CatÃ¡logo de productos

### Datasets Medium (~100 registros cada uno)
- `sales_data_medium.csv`: Volumen intermedio de ventas
- `customer_data_medium.json`: Base ampliada de clientes
- `products_data_medium.csv`: CatÃ¡logo extendido

### Datasets Large (~5000 registros cada uno)
- `sales_data_large.csv`: SimulaciÃ³n de alto volumen
- `customer_data_large.json`: Base de datos empresarial
- `products_data_large.csv`: CatÃ¡logo completo

Esto permite probar el comportamiento del pipeline con diferentes cargas de trabajo.

## ValidaciÃ³n y Troubleshooting

### Scripts de ValidaciÃ³n

He incluido varios scripts para verificar que todo funcione correctamente:

```bash
# ValidaciÃ³n bÃ¡sica - verifica que servicios estÃ©n up
python validate_simple.py

# ValidaciÃ³n completa - ejecuta el pipeline end-to-end
python validate_complete.py

# ValidaciÃ³n de producciÃ³n - prueba con datasets grandes
python validate_production.py

# Script automatizado que ejecuta todas las validaciones
./run_validation.sh
```

### Problemas Comunes y Soluciones

**"Port already in use"**
```bash
# Verificar quÃ© estÃ¡ usando el puerto
lsof -i :8080

# Si es necesario, cambiar puertos en docker-compose.yml
```

**"Container health check failed"**
```bash
# Ver logs especÃ­ficos del servicio que falla
docker-compose logs airflow-webserver

# Reiniciar servicios problemÃ¡ticos
docker-compose restart airflow-webserver
```

**"Out of memory errors"**
```bash
# Verificar uso de recursos
docker stats

# Ajustar memoria en docker-compose.yml si es necesario
# O cerrar otras aplicaciones para liberar RAM
```

**"DAG not appearing in Airflow"**
```bash
# Verificar que el archivo DAG estÃ© correcto
docker-compose logs airflow-scheduler

# Refrescar la pÃ¡gina de Airflow o esperar 30 segundos
# Los DAGs se escanean automÃ¡ticamente cada 30 segundos
```

### Logs Ãštiles

```bash
# Ver todos los logs en tiempo real
./logs.sh

# Logs especÃ­ficos de un servicio
docker-compose logs -f spark-master
docker-compose logs -f airflow-scheduler

# Logs del pipeline de validaciÃ³n
cat data/logs/pipeline_validation.log
```

## ConfiguraciÃ³n Avanzada

### Personalizar Recursos

Puedes ajustar la configuraciÃ³n de recursos editando `docker-compose.yml`:

```yaml
# Ejemplo: Aumentar memoria para Spark
spark-master:
  deploy:
    resources:
      limits:
        memory: 2G
      reservations:
        memory: 1G
```

### Agregar Nuevos Datasets

1. **Coloca tus archivos** en `/data/raw/`
2. **Actualiza el DAG** en `dags/data_pipeline_dag.py` si es necesario
3. **Modifica los scripts Spark** en `/spark/` para manejar tus formatos especÃ­ficos

### Configurar Alertas

El sistema incluye un framework bÃ¡sico de alertas en `monitoring/alert_system.py`. Puedes:

- Configurar notificaciones por email
- Integrar con Slack/Teams
- Conectar con sistemas de monitoreo externos

## CaracterÃ­sticas TÃ©cnicas Destacadas

### Escalabilidad
- **Spark Cluster Distribuido**: FÃ¡cil agregado de workers adicionales
- **Procesamiento Paralelo**: Datos particionados automÃ¡ticamente
- **Resource Management**: Control granular de CPU y memoria

### Confiabilidad
- **Health Checks**: Monitoreo automÃ¡tico de todos los servicios
- **Retry Logic**: Reintentos automÃ¡ticos en tareas que fallan
- **Data Validation**: VerificaciÃ³n de calidad en cada etapa
- **Atomic Operations**: Rollback automÃ¡tico si algo falla

### Observabilidad
- **Logging Estructurado**: Logs en formato JSON para anÃ¡lisis
- **MÃ©tricas de Performance**: Tiempo de ejecuciÃ³n y throughput
- **Monitoring Dashboard**: Interfaces web para monitoreo visual
- **Audit Trail**: Registro completo de todas las operaciones

### Mantenibilidad
- **CÃ³digo Modular**: Cada componente tiene responsabilidades claras
- **DocumentaciÃ³n Exhaustiva**: Comentarios y READMEs detallados
- **Testing Automatizado**: Scripts de validaciÃ³n incluidos
- **Configuration Management**: SeparaciÃ³n clara entre config y cÃ³digo

## PrÃ³ximas Mejoras

### Roadmap TÃ©cnico

**Fase 1: Cloud Native**
- [ ] MigraciÃ³n a Kubernetes con Helm charts
- [ ] IntegraciÃ³n con proveedores cloud (AWS/GCP/Azure)
- [ ] Secretos management con Vault/K8s secrets
- [ ] Auto-scaling basado en mÃ©tricas

**Fase 2: Advanced Analytics**
- [ ] IntegraciÃ³n con Apache Superset para dashboards
- [ ] Machine Learning pipeline con MLflow
- [ ] Real-time streaming con Kafka/Kinesis
- [ ] Data lineage tracking automÃ¡tico

**Fase 3: Enterprise Features**
- [ ] RBAC granular y SSO integration
- [ ] Compliance logging (GDPR, SOX)
- [ ] Multi-tenant architecture
- [ ] Disaster recovery automÃ¡tico

### Optimizaciones Identificadas

**Performance**
- Implementar columnar storage (Parquet) para datasets grandes
- Cache inteligente para consultas repetitivas
- CompresiÃ³n de datos histÃ³ricos
- Query optimization automÃ¡tica

**Operational Excellence**
- CI/CD pipeline para deployment automÃ¡tico
- Infrastructure as Code con Terraform
- Automated testing en pipeline de desarrollo
- Blue/green deployments

## Reflexiones del Desarrollador

### Por QuÃ© Esta Arquitectura

Cuando diseÃ±Ã© este sistema, pensÃ© en los desafÃ­os reales que he visto en empresas:

**Problema**: Los pipelines monolÃ­ticos se vuelven unmaintainable
**SoluciÃ³n**: Microservicios con responsabilidades claras

**Problema**: Scaling vertical tiene lÃ­mites y es costoso
**SoluciÃ³n**: Arquitectura distribuida con scaling horizontal

**Problema**: Debugging en producciÃ³n es complejo sin visibilidad
**SoluciÃ³n**: Logging exhaustivo y interfaces de monitoreo

**Problema**: Onboarding de nuevos developers es lento
**SoluciÃ³n**: DocumentaciÃ³n comprehensive y setup automatizado

### Decisiones de DiseÃ±o

**Docker Compose vs Kubernetes**: Para desarrollo y demos, Docker Compose ofrece simplicidad sin sacrificar funcionalidad. Para producciÃ³n, la migraciÃ³n a K8s serÃ­a straightforward.

**PostgreSQL vs Data Lakes**: Para datos estructurados con queries complejas, PostgreSQL ofrece ACID compliance y performance excelente. Para volÃºmenes masivos no estructurados, se integrarÃ­a S3/HDFS.

**Airflow vs Alternativas**: Airflow tiene el ecosistema mÃ¡s maduro, community support incomparable, y es prÃ¡cticamente el estÃ¡ndar de facto en la industria.

### Lecciones Aprendidas

**Service Orchestration**: La coordinaciÃ³n entre servicios distribuidos requiere healthchecks robustos y manejo cuidadoso de dependencies.

**Resource Balancing**: Ejecutar Spark y Airflow en el mismo host requiere tuning cuidadoso para evitar resource contention.

**Data Quality**: Validar datos en cada etapa del pipeline es crucial - garbage in, garbage out sigue siendo verdad.

**Documentation First**: Escribir documentaciÃ³n mientras develops, no despuÃ©s, resulta en mejor cÃ³digo y menos bugs.

## Contribuciones y Mejoras

Este proyecto refleja mi approach hacia el desarrollo de sistemas de datos enterprise-grade. EstÃ¡ diseÃ±ado siguiendo principios de:

- **Clean Code**: Naming consistente y funciones con responsabilidades Ãºnicas
- **Separation of Concerns**: Cada mÃ³dulo hace una cosa y la hace bien
- **DRY Principle**: ReutilizaciÃ³n de cÃ³digo donde tiene sentido
- **SOLID Principles**: Especialmente Single Responsibility y Dependency Inversion

Si encuentras bugs, tienes sugerencias de mejora, o quieres contribuir con nuevas features, toda contribuciÃ³n es bienvenida. 

### CÃ³mo Contribuir

1. **Reporta Issues**: Usa los templates en GitHub Issues
2. **Sugiere Features**: Describe el use case y business value
3. **Contribuye CÃ³digo**: Fork, develop, test, submit PR
4. **Mejora DocumentaciÃ³n**: Siempre hay espacio para clarificaciones

## Contacto

Este proyecto fue desarrollado como parte de una evaluaciÃ³n tÃ©cnica, pero refleja mi filosofÃ­a de desarrollo: build systems que sean production-ready desde el primer dÃ­a, document everything, y siempre piensa en el siguiente developer que tendrÃ¡ que mantener tu cÃ³digo.

Si tienes preguntas sobre implementaciÃ³n, decisiones de arquitectura, o quieres discutir mejores prÃ¡cticas para pipelines de datos, estarÃ© encantado de conversar.

---

*"La mejor arquitectura es la que resuelve problemas reales y puede evolucionar con el negocio."*

**Desarrollado con y muchas tazas de cafÃ©**
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚  Spark Engine   â”‚
                       â”‚  Data Processor â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Estructura del Proyecto

```
data-pipeline/
â”œâ”€â”€ ğŸ³ docker-compose.yml     # OrquestaciÃ³n de servicios
â”œâ”€â”€ Dockerfile.airflow     # Container Airflow personalizado
â”œâ”€â”€ requirements.txt       # Dependencias Python
â”œâ”€â”€ start.sh              # Script de inicio
â”œâ”€â”€ ğŸ›‘ stop.sh               # Script de parada
â”œâ”€â”€ ğŸ§¹ clean.sh              # Script de limpieza
â”œâ”€â”€ logs.sh               # Visor de logs
â”œâ”€â”€ data/                    # ğŸ“‚ Datos
â”‚   â”œâ”€â”€ raw/                # Datos fuente
â”‚   â”œâ”€â”€ processed/          # Datos procesados
â”‚   â””â”€â”€ logs/               # Logs del sistema
â”œâ”€â”€ dags/                    # DAGs de Airflow
â”œâ”€â”€ spark/                   # Jobs de Spark
â”œâ”€â”€ sql/                     # ğŸ—„Scripts SQL
â”œâ”€â”€ config/                  # Configuraciones
â””â”€â”€ monitoring/              # Monitoreo
```

## Comandos Ãštiles

### GestiÃ³n de Servicios
```bash
# Ver estado de contenedores
docker-compose ps

# Reiniciar un servicio especÃ­fico
docker-compose restart airflow-webserver

# Ejecutar comandos en contenedores
docker-compose exec postgres psql -U airflow -d data_warehouse
docker-compose exec airflow-webserver airflow dags list
```

### Desarrollo
```bash
# Reconstruir imÃ¡genes
docker-compose build --no-cache

# Ver logs de un servicio especÃ­fico
docker-compose logs -f airflow-scheduler

# Acceso directo a PostgreSQL
docker-compose exec postgres psql -U airflow -d data_warehouse
```

## PrÃ³ximos Pasos

1. **Personalizar datos**: Agregar tus CSV/JSON en `data/raw/`
2. **Crear DAGs**: Desarrollar pipelines en `dags/`
3. **Agregar transformaciones**: Usar Spark en `spark/`
4. **Configurar alertas**: Setup en `monitoring/`
5. **Escalar**: Agregar mÃ¡s workers y particionamiento

## Troubleshooting

### Docker no responde
```bash
# Reiniciar Docker Desktop
killall Docker && open -a Docker
```

### Puerto en uso
```bash
# Ver quÃ© estÃ¡ usando el puerto 8080
lsof -i :8080

# Cambiar puerto en docker-compose.yml
# "8080:8080" â†’ "8090:8080"
```

### Fernet Key error
```bash
# Regenerar Fernet Key
python3 -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
```

## ğŸ‰ Â¡Listo para Big Data!

Este pipeline estÃ¡ diseÃ±ado para escalar desde datasets pequeÃ±os hasta terabytes de informaciÃ³n. 

**Â¿Questions?** Revisa los logs con `./logs.sh` o abre un issue.

---
*Hecho con â¤para macOS - Optimizado para Apple Silicon y Intel*
