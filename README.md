# ğŸš€ Data Pipeline Escalable - macOS Edition

Pipeline de datos empresarial completo con Apache Airflow, Spark y PostgreSQL.

## âš¡ Inicio RÃ¡pido

```bash
# Iniciar el pipeline completo
./start.sh

# Ver logs en tiempo real
./logs.sh

# Detener el pipeline
./stop.sh

# Limpiar todo (incluye datos)
./clean.sh
```

## ğŸŒ URLs de Acceso

| Servicio | URL | Credenciales |
|----------|-----|--------------|
| ğŸ›ï¸ **Airflow UI** | http://localhost:8080 | admin/admin123 |
| âš¡ **Spark UI** | http://localhost:8081 | - |
| ğŸ—„ï¸ **PostgreSQL** | localhost:5432 | airflow/airflow123 |

## ğŸ“Š DAG Demo Incluido

El pipeline incluye un DAG de demostraciÃ³n que:

âœ… **Valida datos** de mÃºltiples fuentes (CSV, JSON)  
âœ… **Verifica conexiones** a base de datos  
âœ… **Ejecuta checks** de calidad  
âœ… **Genera reportes** automÃ¡ticos  

### Activar el DAG:
1. Ir a http://localhost:8080
2. Login con `admin`/`admin123`
3. Buscar `data_pipeline_etl_demo`
4. Activar el toggle del DAG
5. Hacer clic en "Trigger DAG"

## ğŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Sources  â”‚â”€â”€â”€â–¶â”‚   Airflow DAG   â”‚â”€â”€â”€â–¶â”‚ Data Warehouse  â”‚
â”‚   (CSV/JSON)    â”‚    â”‚   Orchestrator  â”‚    â”‚   PostgreSQL    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚  Spark Engine   â”‚
                       â”‚  Data Processor â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Estructura del Proyecto

```
data-pipeline/
â”œâ”€â”€ ğŸ³ docker-compose.yml     # OrquestaciÃ³n de servicios
â”œâ”€â”€ ğŸ“¦ Dockerfile.airflow     # Container Airflow personalizado
â”œâ”€â”€ ğŸ“‹ requirements.txt       # Dependencias Python
â”œâ”€â”€ ğŸš€ start.sh              # Script de inicio
â”œâ”€â”€ ğŸ›‘ stop.sh               # Script de parada
â”œâ”€â”€ ğŸ§¹ clean.sh              # Script de limpieza
â”œâ”€â”€ ğŸ“Š logs.sh               # Visor de logs
â”œâ”€â”€ data/                    # ğŸ“‚ Datos
â”‚   â”œâ”€â”€ raw/                # Datos fuente
â”‚   â”œâ”€â”€ processed/          # Datos procesados
â”‚   â””â”€â”€ logs/               # Logs del sistema
â”œâ”€â”€ dags/                    # ğŸ¯ DAGs de Airflow
â”œâ”€â”€ spark/                   # âš¡ Jobs de Spark
â”œâ”€â”€ sql/                     # ğŸ—„ï¸ Scripts SQL
â”œâ”€â”€ config/                  # âš™ï¸ Configuraciones
â””â”€â”€ monitoring/              # ğŸ“ˆ Monitoreo
```

## ğŸ”§ Comandos Ãštiles

### GestiÃ³n de Servicios
```bash
# Ver estado de contenedores
docker-compose ps

# Reiniciar un servicio especÃ­fico
docker-compose restart airflow-webserver

# Ejecutar comandos en contenedores
docker-compose exec postgres psql -U airflow -d data_warehouse
docker-compose exec airflow-webserver airflow dags list
```

### Desarrollo
```bash
# Reconstruir imÃ¡genes
docker-compose build --no-cache

# Ver logs de un servicio especÃ­fico
docker-compose logs -f airflow-scheduler

# Acceso directo a PostgreSQL
docker-compose exec postgres psql -U airflow -d data_warehouse
```

## ğŸ¯ PrÃ³ximos Pasos

1. **Personalizar datos**: Agregar tus CSV/JSON en `data/raw/`
2. **Crear DAGs**: Desarrollar pipelines en `dags/`
3. **Agregar transformaciones**: Usar Spark en `spark/`
4. **Configurar alertas**: Setup en `monitoring/`
5. **Escalar**: Agregar mÃ¡s workers y particionamiento

## ğŸš¨ Troubleshooting

### Docker no responde
```bash
# Reiniciar Docker Desktop
killall Docker && open -a Docker
```

### Puerto en uso
```bash
# Ver quÃ© estÃ¡ usando el puerto 8080
lsof -i :8080

# Cambiar puerto en docker-compose.yml
# "8080:8080" â†’ "8090:8080"
```

### Fernet Key error
```bash
# Regenerar Fernet Key
python3 -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
```

## ğŸ‰ Â¡Listo para Big Data!

Este pipeline estÃ¡ diseÃ±ado para escalar desde datasets pequeÃ±os hasta terabytes de informaciÃ³n. 

**Â¿Questions?** Revisa los logs con `./logs.sh` o abre un issue.

---
*Hecho con â¤ï¸ para macOS - Optimizado para Apple Silicon y Intel*
